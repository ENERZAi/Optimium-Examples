module core.List.List as ll
module common.utils as commonutils
module common.params as params

template</attr_params: params.Attributes, optim_params: params.Optimization, layer_params: ll.List<params.Layerargs>,
    input_data: ll.List<ll.List<ut.UnionTensor>>, input_dtypes: ll.List<ll.List<rtType>>, input_shapes: ll.List<ll.List<i32>>, input_scales: ll.List<ll.List<ut.UnionTensor>>, input_zero_points: ll.List<ll.List<ut.UnionTensor>>,
    output_dtypes: ll.List<ll.List<rtType>>, output_shapes: ll.List<ll.List<i32>>, output_scales: ll.List<ll.List<ut.UnionTensor>>, output_zero_points: ll.List<ll.List<ut.UnionTensor>>,
    input_edges: ll.List<ll.List<tuple<i32, i32>>>, output_edges: ll.List<ll.List<tuple<i32, i32>>>, ismainops: ll.List<boolean>,
    input_t: rtType, input_rt_list : ll.List<rtType>,  output_t: rtType, output_rt_list : ll.List<rtType>/>
attr[Extern : attr_params->name, Optimization : { VectorSize : 2048 }]
fun concat(inputs : input_t, mut &outputs: output_t) -> i32{
    ${
        let pack = ll.item 0 (optim_params->pack)
        let unroll = ll.item 0 (optim_params->unroll)

        let thislayer_params = 
            match (ll.item 0 layer_params) with 
                | params.Concat x -> x
        let algorithm = optim_params->algorithm
        let concat_op_input_shapes = ll.item <| 0 <| input_shapes
        let input_nums = ll.len <| concat_op_input_shapes
        let concat_dim = thislayer_params->dim
        
        let input_dim = ll.len <| (ll.item 0 <| concat_op_input_shapes)

        let last_dim = if((input_dim -1) == concat_dim){
            true
        } else{
            false
        }

        let get_each_fold_shape tensor_shape =
            let front = if(concat_dim == 0){
                1
            } else{
                commonutils.factorial <| (ll.take <| concat_dim <| tensor_shape)
            }
            let back = commonutils.factorial <| (ll.drop <| concat_dim <| tensor_shape)
            [front;back]


        let rec get_fold_shapes index =
            if(index == input_nums){
                []
            } else{
                let fold_shape = get_each_fold_shape <| (ll.item <| index <| concat_op_input_shapes)
                fold_shape ; (get_fold_shapes <| (index+1))
            }

        let reshaped_input_shapes = get_fold_shapes <| 0
        let rec getting_accum_list index = 
            print(index)
            if(index == 0){
                [0]
            } else if (index == input_nums){
                getting_accum_list <| (index-1)
            } else{
                let perv_accum_list = getting_accum_list <| (index-1)
                let prev_idx = ll.item <| 1 <| (ll.item <| (index-1) <| reshaped_input_shapes)
                let prev_accum = ll.item <| 0  <| perv_accum_list
                (prev_idx + prev_accum) ; perv_accum_list
            }
        let accum_offset_list = ll.reverse <| (getting_accum_list <| input_nums)
        let front_dim = ll.item <| 0 <| (ll.item <| 0 <| reshaped_input_shapes)

        let reshaped_inputname = "reshaped_input"

        let output_shape = ll.item <| 0 <| (ll.item <| 0 <| output_shapes)
        let output_front = if (concat_dim == 0){
            1
        } else{
            commonutils.factorial <| (ll.take <| concat_dim <| output_shape)
        }
        let output_back = commonutils.factorial <| (ll.drop <| concat_dim <| output_shape)

        let inputname = "unpacked_input"

        let rec get_inputname_list index =
            if (index == input_nums){
                []
            } else{
                (inputname + toStr(index)) ; (get_inputname_list <| (index+1))
            }


        let rec get_reshape_expr index nextop =
            if(index == input_nums){
                !{
                    let mut reshaped_output = reshapeTo((${output_front}, ${output_back}), &outputs[|0|])
                    ${nextop}

                }
            } else{
                !{
                    // TODO
                    let ${reshaped_inputname + toStr(index)} = reshapeTo(${ll.toRtTuple <| (ll.item <| index <| reshaped_input_shapes)}, inputs[|${index}|]) // 수정 필요
                    // let ${reshaped_inputname + toStr(index)} = reshapeTo(${ll.toRtTuple <| (ll.item <| index <| reshaped_input_shapes)}, ${inputname + toStr(index)})
                    ${get_reshape_expr <| (index+1) <| nextop}
                }
            }
            
        let reshape_expr = get_reshape_expr <| 0


        // concat_dim 이전 기준으로 번갈아가면서 옮기는 알고리즘 - althernative
        let itername = "p"
        let get_each_copy_expr index =
            let backnum = (ll.item <| 1 <| (ll.item <| index <| reshaped_input_shapes))
            let vecrem =  backnum % (pack * unroll)
            let vecname = "vec" + toStr(index)
            let output_offset = ll.item <| index <| accum_offset_list            
            let rec get_vec_expr start end num nested_index = 
                if (nested_index == (num-1)){
                    !{
                        let ${vecname +  toStr(nested_index)} = ${reshaped_inputname + toStr(index)}[ (frontidx, (${start}+${nested_index*pack}) : ${end}:1i,)]
                        // TODO - quantization adjustment
                        reshaped_output[ (frontidx, (${start}+${output_offset}+${nested_index*pack}) : ${output_offset}+${end}:1i,)] <- ${vecname +  toStr(nested_index)}
                    }
                } else{
                    !{
                        let ${vecname + toStr(nested_index)} = ${reshaped_inputname + toStr(index)}[ (frontidx, (${start} + ${nested_index * pack}) : (${start} + ${(nested_index+1) * pack}):1)]
                        reshaped_output[ (frontidx, (${output_offset}+${start} + ${nested_index * pack}) : (${output_offset}+${start} + ${(nested_index+1) * pack}):1)] <- ${vecname + toStr(nested_index)}
                        ${
                            get_vec_expr <| start <| end <| num <| (nested_index+1)
                        }
                    }
                    
                }
            
            let each_itername0 = itername + toStr(index) + toStr(0)
            
            !{
                attr [ Parallel : (backnum >= ((attr_params->num_threads * pack * unroll) + vecrem))]
                // attr [ Parallel : ((front_dim < (attr_params->num_threads)) && ((backnum-vecrem)/pack/unroll >= (attr_params->num_threads)))]
                for(${each_itername0} from 0 to ${backnum - vecrem} step ${pack * unroll}){

                    ${ get_vec_expr <| !{${each_itername0}} <| !{${each_itername0} + ${unroll * pack}} <| unroll <| 0 }
                }
                let _ = ${
                    if (vecrem > 0){
                        if (pack == 1){
                            get_vec_expr <| (backnum - vecrem) <| backnum <| vecrem <| 0 
                        } else{
                            get_vec_expr <| (backnum - vecrem) <| backnum <| ((vecrem-1)/ pack+1) <| 0 
                        }
                    } else{
                        !{0}
                    }
                }
            }

        let rec get_copy_expr index =
            // if(index == 1){
            if(index == input_nums){   
                !{
                    _end_
                }
            } else{
                !{
                    let _ = ${get_each_copy_expr <| index}
                    ${get_copy_expr <| (index+1)}
                }
            }

        let copy_expr = !{
            attr [ Parallel : (front_dim >= attr_params->num_threads)]
            for(frontidx from 0 to ${front_dim} step 1i){
                ${
                    get_copy_expr <| 0
                }
            }
        }

        let last_expr prevop =
            !{
                let _ = ${prevop}
                0
            }

        last_expr <| (reshape_expr <| copy_expr)
    }
    
}