module core.List.List as ll
module common.uniontensor as ut
module common.params as params

template</attr_params: params.Attributes, optim_params: params.Optimization, layer_params: ll.List<params.Layerargs>,
    input_data: ll.List<ll.List<ut.UnionTensor>>, input_dtypes: ll.List<ll.List<rtType>>, input_shapes: ll.List<ll.List<i32>>, input_scales: ll.List<ll.List<ut.UnionTensor>>, input_zero_points: ll.List<ll.List<ut.UnionTensor>>,
    output_dtypes: ll.List<ll.List<rtType>>, output_shapes: ll.List<ll.List<i32>>, output_scales: ll.List<ll.List<ut.UnionTensor>>, output_zero_points: ll.List<ll.List<ut.UnionTensor>>,
    input_edges: ll.List<ll.List<tuple<i32, i32>>>, output_edges: ll.List<ll.List<tuple<i32, i32>>>, ismainops: ll.List<boolean>,
    input_t: rtType, input_rt_list : ll.List<rtType>,  output_t: rtType, output_rt_list : ll.List<rtType>/>
attr[Extern : attr_params->name, Optimization : { VectorSize : 512 }]
fun space_to_depth(inputs: input_t, mut &outputs: output_t) -> i32 {
    ${  
        let layerindex = (
            let mut index = 0
            for(i from 0 to (ll.len layer_params)){
                match (ll.item i layer_params) with 
                    | params.Space_to_depth x   -> if((ll.item i ismainops)){index <- i} else{_end_}
                    | _                 -> _end_
            }
            index
        )
        let thislayer_params = 
            match (ll.item index layer_params) with
                | params.Space_to_depth x -> x
        
        let input_dtype = ll.item 0 (ll.item layerindex input_dtypes)
        let input_shape = ll.item 0 (ll.item layerindex input_shapes)
        let input_scales = ll.item 0 (ll.item layerindex input_scales)
        let input_zero_points = ll.item 0 (ll.item layerindex input_zero_points)
        let output_dtype = ll.item 0 (ll.item layerindex output_dtypes)
        let output_shape = ll.item 0 (ll.item layerindex output_shapes)
        let output_scales = ll.item 0 (ll.item layerindex output_scales)
        let output_zero_points = ll.item 0 (ll.item layerindex output_zero_points)
        let pack = ll.item 0 (optim_params->pack)
        let unroll = ll.item 0 (optim_params->unroll)

        let block_size = thislayer_params->block_size
        
        let h = ll.item 1 input_shape
        let w = ll.item 2 input_shape 
        let c = ll.item 3 input_shape 

        let _ = if(h % block_size != 0){
            except("Dimension size must be evenly divisible by (" + toStr(block_size) + ") but is (" + toStr(h) + ")")
        } else if(w % block_size != 0) {
            except("Dimension size must be evenly divisible by (" + toStr(block_size) + ") but is (" + toStr(w) + ")")
        }
        else{
            _end_
        }

        let rec shape_size index shape data =
            if(index == ll.len shape){
                data
                
            } else{
                shape_size (index+1) shape (data*(ll.item index shape))
            }
        let input_size = shape_size 0 input_shape 1
        let output_size = shape_size 0 output_shape 1
        
        let step_size = (w/block_size)
        let bstep_size = (block_size*c)
        let wc = (w*c)
        let base_size = w*c*block_size
        let hstep_size = h/block_size
        !{
            let mut batchoutput = &outputs[|0|]
            let batchinput = inputs[|0|]
            let flat_input = reshapeTo(${(input_size,)}, batchinput)
            let mut flat_output = reshapeTo((${output_size},),&batchoutput)
            for(idx1 from 0 to ${hstep_size}){
                for(idx2 from 0 to ${step_size}){
                    for(idx3 from 0 to ${block_size}){
                        flat_output[((idx1*${w}+idx2*${block_size}+idx3)*${bstep_size}:((idx1*${w}+idx2*${block_size}+idx3)*${bstep_size})+${bstep_size}:1i,)]<- flat_input[(idx1*${base_size}+idx2*${bstep_size}+idx3*${wc}:idx1*${base_size}+idx2*${bstep_size}+idx3*${wc}+${bstep_size}:1i,)]
                        _end_
                    }
                    _end_
                }
                _end_
            }
            0
        }
        
    }

     
}

