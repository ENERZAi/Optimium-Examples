module core.List.List as ll
module common.uniontensor as ut
module common.params as params

template</attr_params: params.Attributes, optim_params: params.Optimization, layer_params: ll.List<params.Layerargs>,
    input_data: ll.List<ll.List<ut.UnionTensor>>, input_dtypes: ll.List<ll.List<rtType>>, input_shapes: ll.List<ll.List<i32>>, input_scales: ll.List<ll.List<ut.UnionTensor>>, input_zero_points: ll.List<ll.List<ut.UnionTensor>>,
    output_dtypes: ll.List<ll.List<rtType>>, output_shapes: ll.List<ll.List<i32>>, output_scales: ll.List<ll.List<ut.UnionTensor>>, output_zero_points: ll.List<ll.List<ut.UnionTensor>>,
    input_edges: ll.List<ll.List<tuple<i32, i32>>>, output_edges: ll.List<ll.List<tuple<i32, i32>>>, ismainops: ll.List<boolean>,
    input_t: rtType, input_rt_list : ll.List<rtType>,  output_t: rtType, output_rt_list : ll.List<rtType>/>
attr[Extern : attr_params->name, Optimization : { VectorSize : 512 }]
fun sigmoid(inputs: input_t, mut &outputs: output_t) -> i32 {
    ${
        let input_dtype = ll.item 0 (ll.item 0 input_dtypes)
        let input_shape = ll.item 0 (ll.item 0 input_shapes)
        let input_scales = ll.item 0 (ll.item 0 input_scales)
        let input_zero_points = ll.item 0 (ll.item 0 input_zero_points)
        let output_dtype = ll.item 0 (ll.item 0 output_dtypes)
        let output_shape = ll.item 0 (ll.item 0 output_shapes)
        let output_scales = ll.item 0 (ll.item 0 output_scales)
        let output_zero_points = ll.item 0 (ll.item 0 output_zero_points)

        let rec shape_size index shape data =
            if (index == (ll.len shape)) {
                data
            } else {
                shape_size (index+1) shape (data*(ll.item index shape))
            }

        let input_size = shape_size 0 input_shape 1
        let output_size = shape_size 0 output_shape 1

        let pack = ll.item 0 (optim_params->pack)
        let unroll = ll.item 0 (optim_params->unroll)

        let lastshape = ll.item (ll.len input_shape - 1) input_shape
        let _ = if (pack * unroll > lastshape) {
            except("pack * unroll(" + toStr(pack * unroll) + ") should be larger than one of the shape of last dimension of input(" + toStr(lastshape) + ")")
        } else {
            _end_
        }

        for (i from 0 to (ll.len output_shape)) {
            if (ll.item i output_shape == ll.item i input_shape) {
                _end_
            } else {
                except("output shape(" + toStr(output_shape) + ") is not compatible with expected shape(" + toStr(input_shape) + "}")
            }
        }

        let tiles =
            if (input_size % pack == 0) {
                [(0, input_size); (input_size, input_size)]
            } else {
                let end1 = input_size - (input_size % pack)
                [(0, end1); (end1, input_size)]
            }

        let (start, end) = ll.item 0 tiles
        let (start2, end2) = ll.item 1 tiles

        let get_constants size =
            if (input_dtype == rtType(f32)) {
                !{
                    let sign_mask = tensor((${size},), -0.0f)
                    let magic_bias = tensor((${size},), 12583039.000000f) // 2 ^ 24 - 2 ^ 22 + 127
                    let log2e = tensor((${size},), 1.4426950216293335f)
                    let minus_ln2 = tensor((${size},), -0.6931471824645996f)
                    let denorm_cutoff = tensor((${size},), -87.33654022216797f) // f32

                    let c1 = tensor((${size},), 0.9999997019767761f)
                    let c2 = tensor((${size},), 0.4999915063381195f)
                    let c3 = tensor((${size},), 0.1666765213012695f)
                    let c4 = tensor((${size},), 0.0418978221714497f)
                    let c5 = tensor((${size},), 0.0082892905920744f)

                    let one = tensor((${size},), 1.0f)
                    let zeros = tensor((${size},), 0.0f)
                    let shifts = tensor((${size},), 23i)
                    let rshifts = tensor((${size},), 31i)

                    (&sign_mask, &magic_bias, &log2e, &minus_ln2, &denorm_cutoff, &c1, &c2, &c3, &c4, &c5, &one, &zeros, &shifts, &rshifts)
                }
            } else if (input_dtype == rtType(f16)) {
                !{
                    let sign_mask = tensor((${size},), -0.0f)
                    let magic_bias = tensor((${size},), 12583039.000000f) // 2 ^ 24 - 2 ^ 22 + 127
                    let log2e = tensor((${size},), 1.4426950216293335f)
                    let minus_ln2 = tensor((${size},), -0.6931471824645996f)
                    let denorm_cutoff = tensor((${size},), -9.703125f)

                    let c1 = tensor((${size},), 1.0f)
                    let c2 = tensor((${size},), 0.5000161528587341f)
                    let c3 = tensor((${size},), 0.166667178273201f)

                    let one = tensor((${size},), 1.0f)
                    let zeros = tensor((${size},), 0.0f)
                    let shifts = tensor((${size},), 23i)
                    let rshifts = tensor((${size},), 31i)

                    (&sign_mask, &magic_bias, &log2e, &minus_ln2, &denorm_cutoff, &c1, &c2, &c3, &one, &zeros, &shifts, &rshifts)
                }
            } else {
                except("unsupported type: only f32, f16 supported")
            }

        let calculate_sigmoid =
            if (input_dtype == rtType(f32)) {
                !{
                    let z = bitcast<f32>(bitcast<i32>(x) | bitcast<i32>(sign_mask))
                    let mut n = z * log2e + magic_bias

                    let s = bitcast<f32>(bitcast<i32>(n) << shifts)

                    n <- -(magic_bias -n)

                    let mut t = n * minus_ln2 + z

                    let mut p = c5 * t + c4 // 5th order approximation
                    p <- p * t + c3
                    p <- p * t + c2
                    p <- p * t + c1

                    t <- t * s
                    let e = t * p + s
                    let d = e + one
                    let f1 = e / d

                    let is_cutoff = cast<f32>(cast<i32>(z < denorm_cutoff))
                    let f2 = is_cutoff * zeros + (one - is_cutoff) * f1

                    let is_positive = one - cast<f32>(bitcast<u32>(bitcast<i32>(x) & bitcast<i32>(sign_mask)) >> rshifts)
                    // let f3 = is_positive * f2 + (one - is_positive) * (one - f2)
                    let f3 = (tensor((1,), -2.0f32) * is_positive + one) * f2 + is_positive
                    &f3
                }
            } else if (input_dtype == rtType(f16)) {
                !{
                    let z = bitcast<f32>(bitcast<i32>(x) | bitcast<i32>(sign_mask))
                    let mut n = z * log2e + magic_bias

                    let s = bitcast<f32>(bitcast<i32>(n) << shifts)

                    n <- -(magic_bias - n)

                    let mut t = n * minus_ln2 + z

                    let mut p = c3 * t + c2 // 5th order approximation
                    p <- p * t + c1

                    t <- t * s
                    let e = t * p + s
                    let d = e + one
                    let f1 = e / d

                    let is_cutoff = cast<f32>(cast<i32>(z < denorm_cutoff))
                    let f2 = is_cutoff * zeros + (one - is_cutoff) * f1

                    let is_positive = one - cast<f32>(bitcast<u32>(bitcast<i32>(x) & bitcast<i32>(sign_mask)) >> rshifts)
                    // let f3 = is_positive * f2 + (one - is_positive) * (one - f2)
                    let f3 = (tensor((1,), -2.0f32) * is_positive + one) * f2 + is_positive
                    &f3
                }
            } else {
                except("unsupported type: only f32, f16 supported")
            }

        if (input_dtype == rtType(f32)) {
            let expr1 =
                !{
                    let (sign_mask, magic_bias, log2e, minus_ln2, denorm_cutoff, c1, c2, c3, c4, c5, one, zeros, shifts, rshifts) = ${get_constants pack}

                    let x = input_reshaped[(idx0:idx0+${pack}:1i,)] // f32

                    let xp = ${calculate_sigmoid}

                    output_reshaped[(idx0:idx0+${pack}:1i,)] <- xp // f32
                }
            let expr2 =
                if (end2 - start2 > 0) {
                    !{
                        let (sign_mask, magic_bias, log2e, minus_ln2, denorm_cutoff, c1, c2, c3, c4, c5, one, zeros, shifts, rshifts) = ${get_constants (end2 -start2)}

                        let x = input_reshaped[(${start2}:${end2}:1i,)] // f32

                        let xp = ${calculate_sigmoid}

                        output_reshaped[(${start2}:${end2}:1i,)]  <- xp
                        0
                    }
                } else {
                    !{0}
                }
            !{
                let mut batchoutput = &outputs[|0|]
                let batchinput = inputs[|0|]

                let input_reshaped = reshapeTo((${input_size},), &batchinput)
                let mut output_reshaped = reshapeTo((${output_size},), &batchoutput)

                for (idx0 from 0 to ${start2} step ${pack}) {
                    ${expr1}
                }

                ${expr2}
            }
        } else if (input_dtype == rtType(f16)) {
            let expr1 =
                !{
                    let (sign_mask, magic_bias, log2e, minus_ln2, denorm_cutoff, c1, c2, c3, one, zeros, shifts, rshifts) = ${get_constants pack}

                    let x = cast<f32>(input_reshaped[(idx0:idx0+${pack}:1i,)])

                    let xp = ${calculate_sigmoid}

                    output_reshaped[(idx0:idx0+${pack}:1i,)] <- cast<f16>(xp)
                }
            let expr2 =
                if (end2 - start2 > 0) {
                    !{
                        let (sign_mask, magic_bias, log2e, minus_ln2, denorm_cutoff, c1, c2, c3, one, zeros, shifts, rshifts) = ${get_constants (end2 -start2)}

                        let x = cast<f32>(input_reshaped[(${start2}:${end2}:1i,)])

                        let xp = ${calculate_sigmoid}

                        output_reshaped[(${start2}:${end2}:1i,)]  <- cast<f16>(xp)
                        0
                    }
                } else {
                    !{0}
                }
            !{
                let mut batchoutput = &outputs[|0|]
                let batchinput = inputs[|0|]

                let input_reshaped = reshapeTo((${input_size},), &batchinput)
                let mut output_reshaped = reshapeTo((${output_size},), &batchoutput)

                for (idx0 from 0 to ${start2} step ${pack}) {
                    ${expr1}
                }

                ${expr2}
            }
        } else {
            except("unsupported type: only f32, f16 supported")
        }
    }
}
